\documentclass{../notatki}
\geometry{margin=1.5cm}
\usepackage[fontsize=6pt]{scrextend}
\usepackage{multicol}

\title{Konspekt kompresja}

\begin{document}

\begin{multicols}{2}

  \section{Wzory}

  Informacja zdarzenia $A$:
  $$
  I(A) = -\log_xP(A)
  $$
  Entropia źródła $X$ ze zdarzeniami $A_1, \dots, A_n$:
  $$
  H(X) = \sum_{i=1}^{n}P(A_i)I(A_i)
  $$
  Średnia długość kodu $\mathcal{C}$:
  $$
  I(\mathcal{C}) = \sum_{i=1}^{n}P(\mathcal{C}_i) \cdot l_i
  $$
  Nierówność Krafta (warunek konieczny jednoznacznej dekodowalności):
  $$
  K(\mathcal{C}) = \sum_{i=1}^{n}2^{-l_i} \leq 1
  $$
  Współczynnik informacji kodu $\mathcal{C}$:
  $$
  \frac{1}{n}\log|\mathcal{C}|
  $$

  \section{Kod Huffmana}

  Znajdź dwa najrzadziej występujące elementy i połącz je w jeden element
  o prawdopodobieństwie $p_1 + p_2$. Rozróżnij je $0$ lub $1$. Powtórz
  ten krok na liście $n-1$ długiej aż zostanie jeden element.

  Jeśli nie znamy prawdopodobieństw, to możemy drzewo tworzyć
  dynamicznie, traktując
  ilość wystąpień jako wagę, które łączymy tworząc poddrzewa.

  \section{Kod Shannon-Fano}

  Dla symboli $a_1, \dots, a_n$ o prawdopodobieństwach $p_1, \dots, p_n$,
  ustalmy kody długości $l_n = \lceil - \log p_i \rceil$. Następnie
  zdefiniujmy zmienne pomocnicze $w_1, \dots w_n$ jako:
  $$
  w_1 = 0, w_j = \sum_{i=1}^{j-1}2^{l_j - l_i}
  $$
  Jeżeli $\lceil \log w_j \rceil = l_j$ to j-te słowo kodowe jest binarną
  reprezentacją $w_j$. Jeżeli $\lceil \log w_j \rceil < l_j$ to reprezentację
  uzupełniamy zerami z lewej strony.

  \section{Kod Tunstalla}

  Chcemy stworzyć kod na $n$ bitach dla $a_1, \dots, a_m$ symboli o
  prawdopodobieństwach $p_1, \dots, p_m$. Tworzenie kodu Tunstalla polega na
  iteracyjnym wyborze ze zbioru symbolu o największym prawdopodobieństwie $S$
  i łączenie go z wszystkimi innymi symbolami tworząc symbole $Sa_m$, nadając im
  prawdopodobieństwa $P \cdot p_m$. Proces ten powtarzamy aż do uzyskania
  kodu o długości $n$.

  \section{Kodowanie Eliasa}

  $$
  n = \lfloor \log_2(x) \rfloor + 1
  $$

  \subsection{\texorpdfstring{$\gamma$}{Gamma}}

  $$
  \gamma(x) = 0^{n-1}(x)_2
  $$

  \subsection{\texorpdfstring{$\delta$}{Delta}}

  $$
  \delta(x) = \gamma(n) + (x)_2
  $$

  \subsection{\texorpdfstring{$\omega$}{Omega}}

  Na koniec umieszczane jest $0$, potem kodowana jest liczba $k=x$. Potem ten
  krok jest powtarzany dla $k=n - 1$ gdzie n to liczba bitów z
  poprzedniego kroku.
  $$
  \omega(x) = \omega(n - 1) + (x)_2 + 0
  $$

  \section{Kodowanie Fibonacciego}

  $$
  f_0=f_1=1
  $$
  $$
  f_n = f_{n-1} + f_{n-2}: n \geq 2
  $$
  $$
  x = \sum_{i=0} a_i \cdot f_i, a_i \in \{0,1\}
  $$

  \section{Kodowanie arytmetyczne}
  \begin{multicols}{2}
    \begin{itemize}
      \item $d = p - l$
      \item $p = l + d \cdot F(j + 1)$
      \item $l = l + d \cdot F(j)$
    \end{itemize}

    \begin{itemize}
      \item $d = p - l$
      \item Wybieramy takie $j$, że $F(j) \leq \frac{x - l}{d} < F(j + 1)$
      \item $p = l + d \cdot F(j + 1)$
      \item $l = l + d \cdot F(j)$
    \end{itemize}
  \end{multicols}

  \section{Kodowanie słownikowe}

  \subsection{LZ77}

  $$
  (o, l, k) = \mathcal{C}_{i - o} \cdots \mathcal{C}_{i - o + l} k
  $$

  \subsection{LZ78}

  \begin{enumerate}
    \item Szukaj w słowniku najdłuższy prefiks aktualnego okna, jeśli
      nie znajdziesz to użyj $\epsilon$.
    \item Dodaj prefiks $+$ znak do słownika.
    \item Zakoduj symbol jako $(i, k)$, gdzie $i$ to numer
      prefiksu w słowniku, a $k$ to symbol.
  \end{enumerate}

  $$
  (i, k) = s(i) + k
  $$

  \subsection{LZW}

  Podobne do LZ78, tylko że zaczynamy ze słownikiem.
  $$
  (i) = s(i)
  $$
  Jeśli napotkasz symbol, którego nie ma w słowniku, ale masz jego prefiks,
  np.: $s(5) = ab?$, to $?$ to pierwsza litera $s(5)$.

  \section{bzip2/BWT}

  Układamy tabelę z dwoma kolumnami. Pierwsza kolumna to słowo posortowane
  leksykograficznie. Druga kolumna to poprzedni znak. Na podstawie tej tabeli
  zapisujemy ostatnią kolumnę, i numer wiersza w którym w pierwszej kolumnie
  znajduje się początek słowa, a w drugiej kolumnie jego koniec.

  \begin{center}

    \begin{tabular}{|c|c|}
      \hline
      e & h \\
      \hline
      \rowcolor{gray!50}
      h & o \\
      \hline
      ll & e \\
      \hline
      lo & l \\
      \hline
      o & l \\
      \hline
    \end{tabular}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      \rowcolor{gray!50}
      0 & 1 & 2 & 3 & 4 \\
      \hline
      e & h & l & l & o \\
      \hline
      2 & 0 & 3 & 4 & 1 \\
      \hline
    \end{tabular}
  \end{center}

  \section{Move To Front}

  Jest to transformacja zmniejszająca entropię. Zaczynamy od tabeli liter
  ze słowa posortowanych alfabetycznie. Następnie dla każdej litery ze słowa
  kodujemy jej pozycję w tabeli, a następnie przesuwamy ją na początek
  tabeli. W ten sposób hello to 11203.

  \section{PPM}

  Dla każdego symbolu z tekstu, sprawdzamy jego kontekst, zakładając daną
  maksymalną długość kontekstu. Kolejno sprawdzamy drzewa kontekstowe dla
  długości $n$, $n-1$, $\dots$, $0$ gdzie interesuje nas tylko to ile razy
  dany symbol występuje w tekście, aż dojdziemy do kontekstu $-1$ gdzie
  zaznaczamy tylko czy symbol występuje czy nie. Następnie na podstawie
  takiej serii drzew, można zbudować solidny kod, chociażby Huffmana.

  \resizebox{0.9\columnwidth}{!}{
    \begin{minipage}{0.7\textwidth}
      \centering
      \begin{tabular}{|c|}
        \hline
        Symbol \\ \hline
        t \\ \hline
        h \\ \hline
        i \\ \hline
        s \\ \hline
        - \\ \hline
      \end{tabular}
      \begin{tabular}{c|c}
        Symbol & Licznik \\ \hline
        \texttt{ESC} & 1 \\ \hline
        t & 1 \\ \hline
        h & 1 \\ \hline
        i & 1 \\ \hline
        s & 2 \\ \hline
        - & 1 \\
      \end{tabular}
      \begin{tabular}{c|c|c}
        Kontekst & Symbol & Licznik \\
        \hline
        t & \texttt{ESC} & 1 \\
        & h & 1 \\
        \hline
        h & \texttt{ESC} & 1 \\
        & i & 1 \\
        \hline
        i & \texttt{ESC} & 1 \\
        & s & 2 \\
        \hline
        s & \texttt{ESC} & 1 \\
        & - & 1 \\
        \hline
        - & \texttt{ESC} & 1 \\
        & i & 1 \\
      \end{tabular}
      \begin{tabular}{c|c|c}
        Kontekst & Symbol & Licznik \\
        \hline
        th & \texttt{ESC} & 1 \\
        & i & 1 \\
        \hline
        hi & \texttt{ESC} & 1 \\
        & s & 1 \\
        \hline
        is & \texttt{ESC} & 1 \\
        & - & 1 \\
        \hline
        s- & \texttt{ESC} & 1 \\
        & i & 1 \\
        \hline
        -i & \texttt{ESC} & 1 \\
        & s & 1 \\
      \end{tabular}
    \end{minipage}
  }

  \section{Kody Hamminga}

  Dla długości kodu $n = 2^m - 1$, mają liczbę bitów informacji $k = n - m$.
  Macierz parzystości, powstaje poprzez zapis binarnych reprezentacji liczb
  $1, \dots, n$ w postaci kolumn.
  $$
  H_H(3) =
  \begin{bmatrix}
    0 & 0 & 0 & 1 & 1 & 1 & 1 \\
    0 & 1 & 1 & 0 & 0 & 1 & 1 \\
    1 & 0 & 1 & 0 & 1 & 0 & 1 \\
  \end{bmatrix}
  $$
  Aby stworzyć macierz generującą, musimy potraktować $H_H$ jako
  macierz równania, gdzie pierwsze $k$ wierszy to macierz identyczności,
  a pozostałe to przekształcone równania z macierzy parzystości.
  $$
  G_H(3) =
  \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
    0 & 1 & 1 & 1 \\
    1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 1 \\
  \end{bmatrix}
  $$

\end{multicols}

\end{document}