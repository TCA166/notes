\documentclass{../notatki}

\title{Wprowadzenie do teorii sieci neuronowych}

\begin{document}

\tableofcontents

\section{Model McCullocha-Pittsa}

Jest to model matematyczny mający naśladować działanie fizjologicznych neuronów.
Składa się on z $n$ wejść $u_i$ o wagach $w_i$ i jednego wyjścia $y$.
Neuron aktywuje się, gdy suma iloczynów wejść i wag jest większa od
pewnej wartości progowej $\theta$.
$$
n_i,y \in \{0.0, 1.0\} \subset \mathbb{R}
$$
$$
w_i, \theta \in \mathbb{R}
$$
$$
f(x) =
\begin{cases}
  0 & x < 0\\
  1 & x \geq 0
\end{cases}
$$
$$
y(\vec{u}, \vec{w}) = f((\sum_{i=1}^{n}w_iu_i) - \theta)
$$
\begin{figure*}[h]
  \centering
  \begin{tikzpicture}[node distance=0.7cm]
    \node[circle, draw=black] (neuron) {$f$};
    \node (u1) [above left=of neuron] {$u_{i + 1}$};
    \node (dots) [left=of neuron] {$\vdots$};
    \node (u2) [below left=of neuron] {$u_i$};
    \draw[->] (u1) -- (neuron) node[midway, right] {$w_{i + 1}$};
    \draw[->] (u2) -- (neuron) node[midway, right] {$w_i$};
    \draw[->] (neuron) -- ++(1, 0) node[right] {$y$};
  \end{tikzpicture}
  \caption{Wizualizacja modelu McCullocha-Pittsa}
\end{figure*}

\subsection{Przykład}

\begin{table}[h]
  \centering
  \begin{tabular}{|c|c|c|c|}
    \hline
    $u_1$ & $u_2$ & $y$\\
    \hline
    0 & 0 & 0\\
    0 & 1 & 0\\
    1 & 0 & 0\\
    1 & 1 & 1\\
    \hline
  \end{tabular}
  \caption{Tabela prawdy dla funkcji logicznej AND}
\end{table}

\begin{itemize}
  \item $u_1 = u_2 = 0 \rightarrow y = 0 = f(-\theta) \leftrightarrow
    \theta \geq 0$
  \item $u_1 = 0, u_2 = 1 \rightarrow y = 0 = f(w_2 - \theta)
    \leftrightarrow w_2 < \theta$
  \item $u_1 = 1, u_2 = 0 \rightarrow y = 0 = f(w_1 - \theta)
    \leftrightarrow w_1 < \theta$
  \item $u_1 = u_2 = 1 \rightarrow y = 1 = f(w_1 + w_2 - \theta)
    \leftrightarrow w_1 + w_2 \geq \theta$
\end{itemize}

$$
\theta = 3, w_1 = 2, w_2 = 2
$$

\subsection{Reprezentacja wektorowa}

$$
\vec{u} = (u_1, u_2, \ldots, u_n)
$$
$$
\vec{w} = (w_1, w_2, \ldots, w_n)
$$
$$
y(\vec{u}, \vec{w}) = f(\vec{w} \cdot \vec{u} - \theta)
$$

\section{Liniowa separowalność}

$$
U_- = \{\vec{u_1}, \dots, \vec{u_n}\} \subset \mathbb{R}^n
$$
$$
U_+ = \{\vec{u_{n + 1}}, \dots, \vec{u_{n + m}}\} \subset \mathbb{R}^n
$$
$$
U_- \cap U_+ = \emptyset
$$
Mówimy, że zbiory wektorów (wejść) $U_-$ i $U_+$ są liniowo separowalne, jeśli
istnieje jakikolwiek $\vec{w}$ taki, że: $\vec{w} \cdot \vec{u} < 0:
\vec{u} \in U_-$ oraz $\vec{w} \cdot \vec{u} > 0: \vec{u} \in U_+$. Innymi słowy
jeśli istnieje hiperpłaszczyzna, która dzieli zbiory $U_-$ i $U_+$.

\subsection{Przykład}

Dla bramki AND mamy:
$$
U_- = \{(0, 0), (0, 1), (1, 0)\}, U_+ = \{(1, 1)\}
$$

$$
\vec{w} = (2, 2), \theta = 3
$$

\begin{figure*}[h]
  \centering
  \begin{tikzpicture}
    \node[circle, fill=red, radius=0.1] (u1) at (0, 0) {};
    \node[circle, fill=red] (u2) at (0, 2) {};
    \node[circle, fill=red] (u3) at (2, 0) {};
    \node[circle, fill=blue] (v1) at (2, 2) {};
    \draw[->] (-0.5, 0) -- (4, 0) node[right] {$u_1$};
    \draw[->] (0, -0.5) -- (0, 4) node[above] {$u_2$};
    \draw[->] (1.5, 1.5) -- (v1) node[midway, above left] {$\vec{w}$};
    \draw[dashed] (-0.5, 3.5) -- (3.5, -0.5);
  \end{tikzpicture}
  \caption{Liniowa separowalność dla bramki AND}
\end{figure*}

\section{Associatron}

Jest to model pamięci asocjacyjnej, skojarzeniowej, który pozwala na kojarzenie
danych. W tym modelu dane reprezentujemy wektorami binarnymi. Jako, że de facto
tworzymy macierz, to dowolne dane można asocjować.

\subsection{Liniowy model pamięci}

$$
U = \{\vec{u_1}, \vec{u_2}, \dots, \vec{u_n}\} \in \mathbb{R}^n
$$

$$
\vec{u_t} = (u_{t1}, u_{t2}, \dots, u_{tn}), u_{ti} \in \{0, 1\}
$$

\subsection{Cel}

Celem modelu jest stworzenie funkcji, która dla danego wektora $\vec{u_t}$
zwróci wektor $\vec{u_s}$, który jest najbardziej podobny do $\vec{u_t}$.

$$
\varphi : U \rightarrow Y
$$
$$
U, Y \in \mathbb{R}^n
$$

\subsection{Macierz wag}

$$
\vec{y_t} = W \cdot \vec{u_t}
$$
$$
W = [w_{ij}] = \frac{1}{n} \sum_{t = 1}^{N} \vec{y_t} \cdot \vec{u_t}^T
$$
Macierz wag jest stała dla danego zbioru wektorów $U$.

\subsection{Liniowa funkcja asocjacyjna}

Zakładając, że wszystke wektory w $U$ są ortogonalne ($\vec{u_t} \bot
\vec{u_s} \leftrightarrow \vec{u_t} \cdot \vec{u_s} = 0$) to wówczas:
$$
\varphi(\vec{u_i}) = W \cdot \vec{u_i} = \frac{1}{n} \sum_{\vec{u_t} \in U}
\vec{y_t}(\vec{u_t}\cdot \vec{u_i}) = \frac{1}{n} \cdot \vec{y_i}
\cdot \vec{u_i} \cdot \vec{u_i} = \frac{1}{n} \cdot \vec{y_i} \cdot n
= \vec{y_i}
$$
Czyli dla każdego wektora $\vec{u_i}$ zwracamy wektor $\vec{y_i}$.
W istocie w powyższym równaniu szukamy takiego wektora w $U$, który
po pomnożeniu przez wejście, nie będzie ortogonalny, czyli zwróci $n$.
$$
\vec{u_t} \cdot \vec{u_t} = u_t^Tu_t =
\begin{bmatrix}
  1 \\ 1 \\ \vdots \\ 1
\end{bmatrix} \cdot
\begin{bmatrix}
  1 & 1 & \dots & 1
\end{bmatrix} = n
$$

\subsection{Nieliniowa funkcja asocjacyjna}

Ograniczenie, że wektory w $U$ są ortogonalne jest bardzo silne.
W praktyce nie jesteśmy w stanie tego założyć. Zatem możemy wprowadzić
funkcję $\varphi'$, która pozwala na pewnego rodzaju błąd.
$$
\varphi'(\vec{u_t}) =
\begin{bmatrix}
  sgn(x_1) \\ sgn(x_2) \\ \vdots \\ sgn(x_n)
\end{bmatrix}, \vec{x} = W \cdot \vec{u_t}
$$
$$
sgn(x) =
\begin{cases}
  -1 & x < 0\\
  1 & x \geq 0
\end{cases}
$$

\section{Gradient Descent}

Metoda gradient descent pozwala na znalezienie lokalnego minimum funkcji.
Gradient funkcji to wektor pochodnych cząstkowych
funkcji po każdej zmiennej. Dla funkcji jednej zmiennej gradient to pochodna
funkcji po tej zmiennej.
$$
\nabla F = \left(\frac{\partial F}{\partial x_1}, \frac{\partial F}{\partial
x_2}, \dots, \frac{\partial F}{\partial x_n}\right)
$$

W metodzie gradient descent zaczynamy od pewnego punktu (wektora) i iteracyjnie
zmieniamy ten wektor w kierunku przeciwnym do gradientu funkcji. Można
to sobie wyobrazić jako stanie na górce i chodzenie w kierunku największego
spadku.
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \draw[->] (-2, 0) -- (3, 0) node[right] {$x$};
    \draw[->] (0, -0.5) -- (0, 4) node[above] {$y$};
    \draw[domain=-1:2, smooth, variable=\x, red] plot ({\x}, {(\x)^3 - (\x
    )^2 -(\x) + 1.5}) node[right] {$F(x)$};

    \draw[blue] (0.7, 0.5) -- (1.3, 0.5) node[right] {$\frac{\partial
    F}{\partial x}F(x)=0$};
    \node[fill=blue, minimum size=2pt, inner sep=0pt] at (1, 0.5) {};

  \end{tikzpicture}
  \caption{Przykład działania metody gradient descent}
\end{figure}

\subsection{Algorytm}

Instancję algorytmu określa wektor początkowy $\vec{x_0}$, stała $c$ oraz stała
$\epsilon$.
Dla wektora początkowego $\vec{x} = (x_1, x_2, \dots x_n)$, oraz wyjściowego
po jednym kroku $\vec{x'}$ tak długo jak $\max_{1 \ge i \ge n}|x'_i -
x_i| > \epsilon$ wykonujemy następującą operację:
$$
\vec{x'} = \vec{x} - c \nabla F(\vec{x})
$$
lub równoważnie:
$$
x'_i = x_i - c \frac{\partial F}{\partial x_i}F(x)
$$

\subsection{Problemy}

Największym problemem tej metody, jest określenie odpowiednich wartości stałych
$c$ oraz $\epsilon$. Wartość $c$ nie może być zbyt mała, bo wtedy algorytm
będzie działał bardzo wolno, ale też nie może być zbyt duża, bo wtedy algorytm
może nie zbiegać. Wartość $\epsilon$ określa dokładność, z jaką chcemy znaleźć
minimum. Im mniejsza wartość, tym dokładniejsze minimum, ale też dłuższy czas
działania algorytmu. Z kolei wartość $\vec{x_0}$ jest bardzo ważna, bo od niej
zależy, czy algorytm zbiegnie do minimum lokalnego czy globalnego.

\section{Backpropagation}

Backpropagation (backprop) to metoda trenowania sieci neuronowych. Jest to
w istocie zastosowanie optymalizacji gradientowej do determinowania wag sieci
neuronowej.

Najpierw obliczamy wynik sieci neuronowej. Dla wektora wejściowego $\vec{x}$,
czynnika bias $b$
oraz macierzy początkowej wag $\vec{W_0}$ wykonujemy następujące operacje:
$$
\vec{y} = \sigma(\vec{x} \cdot \vec{W_0} + b)
$$
gdzie $\sigma$ to funkcja aktywacji. $W_0 \in \mathbb{R}^{n \times m}$ gdzie $n$
to liczba wejść, a $m$ to liczba neuronów w warstwie.

Następnie określamy metrykę błędu $F$. Klasycznym błędem jest MSE
($\frac{1}{n} \sum_{i=1}^{n} (y_i - t_i)^2$). Następnie mając
określoną metrykę, dla każdej warstwy obliczamy gradient błędu i to jak każda
waga się przczyniła do błędu. Zakładając, optymalizację SGD, obliczamy gradient
błędu dla każdej warstwy i aktualizujemy wagi za pomocą wzoru:
$$
W_{i+1} = W_i - c \frac{\partial F}{\partial W_i} = W_i - c
\frac{\partial F}{\partial \hat{y}} \cdot \sigma'(W_i x_i + b_i)
$$
gdzie $c$ to współczynnik uczenia.

$$
\frac{\partial MSE}{\partial \hat{y}} = \hat{y} - y
$$

\begin{enumerate}
  \item Forward pass
    \begin{enumerate}
      \item $a = \sigma(W_1x + b_1)$ - pierwsza warstwa
      \item $\hat{y} = \sigma(W_2a + b_2)$ - druga warstwa
      \item $\mathcal{L} = MSE(\hat{y}, y) = \frac{1}{n} \sum_{i=1}^{n}
        (\hat{y}_i - y_i)^2$ - obliczamy błąd
    \end{enumerate}
  \item Backprop
    \begin{enumerate}
      \item $\frac{\partial \mathcal{L}}{\partial b_2} = \delta_2 =
        \frac{\partial \mathcal{L}}{\partial \hat{y}} \cdot
        \sigma'(W_2a + b_2)$
      \item $\frac{\partial \mathcal{L}}{\partial W_2} = \delta_2 \cdot a^T$
      \item $\frac{\partial \mathcal{L}}{\partial b_1} = \delta_1 =
        (W_2^T \delta_2) \cdot \sigma'(W_1x + b_1)$
      \item $\frac{\partial \mathcal{L}}{\partial W_1} = \delta_1 \cdot x^T$
    \end{enumerate}
  \item Descent
    \begin{enumerate}
      \item $W_1 := W_1 - c \frac{\partial \mathcal{L}}{\partial W_1}$
      \item $W_2 := W_2 - c \frac{\partial \mathcal{L}}{\partial W_2}$
      \item $b_1 := b_1 - c \delta_1$
      \item $b_2 := b_2 - c \delta_2$
    \end{enumerate}
\end{enumerate}

\section{CNN}

\subsection{Konwolucja}

Wejściem jest macierz $u_{i,j}$ o wymiarach $N \times N$. Rdzeniem jest macierz
$w_{i,j}$ o wymiarach $2H + 1 \times 2H + 1$. Operacja konwolucji zwraca nam
macierz $x_{i, j}$ o wymiarach $N \times N$. W konwolucji stosuje się funkcję
aktywacji $f$, tutaj:
$$
f(x) =
\begin{cases}
  1 & \text{if } x \geq 0 \\
  0 & \text{otherwise}
\end{cases}
$$

$$
x_{i,j} = f(\sum_{k=-H}^{H} \sum_{l=-H}^{H} u_{i+k,j+l} w_{k,l} - \theta)
$$

\subsection{Pooling}

Wejściem jest zkonwolucjonowana macierz $x_{i, j}$ o wymiarach $N \times N$.
Wyjściem jest macierz $y_{i, j}$ o identycznych wymiarach.

$$
y_{i, j} = f(\frac{1}{(2K + 1)^2} \sum_{k=-K}^{K} \sum_{l=-K}^{K}
x_{i+k,j+l} - \theta)
$$

\section{MB}

Mamy wektor wejściowy $x$ o wymiarach $N$. Wektor jest inicjalizowany losowo.

$$
u_i(t) = \sum_{j=1}^{n} w_{i,j} x_j(t) - \theta_i
$$
$$
f(u_i(t)) = \frac{1}{1 + e^{-u_i(t) / T}}
$$
$$
x_i(t + 1) =
\begin{cases}
  1 & \text{if } f(u_i(t)) \geq U(0, 1) \\
  0 & \text{otherwise}
\end{cases}
$$

\section{Transformer}

Dla każdego słowa mamy wektor wejściowy $x_1$, $x_2$, \dots, $x_n$.
Tworzymy trzy projekcje wektorów wejściowych:
$$
q_i = W_q x_i \\
k_i = W_k x_i \\
v_i = W_v x_i
$$

$$
y_i = \sum_{j=1} \frac{q_i \cdot k_j}{\sqrt{d_k}} \cdot v_j
$$

\end{document}
