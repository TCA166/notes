\documentclass{../notatki}

\title{Konspekt do kolokwium z statystyki}

\begin{document}

\tableofcontents

\section{Rozkłady używane w statystyce}

\subsection{Rozkład normalny}

$$
X \sim N(\mu, \sigma^2)
$$
gdzie $\mu$ to wartość oczekiwana, a $\sigma^2$ to
wariancja. Rozkład normalny jest rozkładem ciągłym, który jest symetryczny
względem średniej. Wartość oczekiwana i mediana są równe.

\begin{lstlisting}[language=R, caption=gęstość w punkcie x]
dnorm(x, mean = 0, sd = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=dystrybuanta]
pnorm(q, mean = 0, sd = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=kwantyl p-tego percentyla]
qnorm(p, mean = 0, sd = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=n losowych zmiennych z rozkładu normalnego]
rnorm(n, mean = 0, sd = 1)
\end{lstlisting}

\subsection{Rozkład \texorpdfstring{$\chi^2$}{chi-kwadrat}}

Jeśli $X_1,\dots, X_n$ są niezależne i $X_{1..n} \sim N(0, 1)$ to:
$$
Z = X_1^2 + X_2^2 + \dots + X_n^2 \sim \chi^2(n)
$$
$$
\mathbb{E} Z = n \quad Var(Z) = 2n
$$

Jeśli zmienne niezależne $X_1, \dots, X_n \sim N(m_n, 1)$ to zmienna losowa też
ma rozkład $\chi^2$, lecz z parametrem niecentralności $m =
\sqrt{m_1^2 + \dots m_n^2}$.
Wtedy $\mathbb{E} Z = n + m \quad Var(Z) = 2(k + 2m)$.

\begin{lstlisting}[language=R, caption=gęstość w punkcie x]
dchisq(x, df = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=dystrybuanta]
pchisq(q, df = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=kwantyl p-tego percentyla]
qchisq(p, df = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=n losowych zmiennych z rozkładu chi-kwadrat]
rchisq(n, df = 1)
\end{lstlisting}

\subsection{Rozkład t-Studenta}

Jeżeli $Z \sim N(0,1), X \sim \chi^2(k)$ to wtedy zmienna:
$$
Y = \frac{Z}{\sqrt{\frac{X}{k}}} \sim t(k)
$$

\begin{lstlisting}[language=R, caption=gęstość w punkcie x]
dt(x, df = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=dystrybuanta]
pt(q, df = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=kwantyl p-tego percentyla]
qt(p, df = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=n losowych zmiennych z rozkładu t-Studenta]
rt(n, df = 1)
\end{lstlisting}

\subsection{Rozkład F-Snedecora}

Jeżeli $X \sim \chi^2(k_1)$ i $Y \sim \chi^2(k_2)$ to wtedy zmienna:
$$
Z = \frac{X/k_1}{Y/k_2} \sim F(k_1, k_2)
$$
$$
\mathbb{E} Z = \frac{k_2}{k_2 - 2} \quad Var(Z) = \frac{2k_2^2(k_1 +
k_2 - 2)}{k_1(k_2 - 2)^2(k_2 - 4)}
$$

\begin{lstlisting}[language=R, caption=gęstość w punkcie x]
df(x, df1 = 1, df2 = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=dystrybuanta]
pf(q, df1 = 1, df2 = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=kwantyl p-tego percentyla]
qf(p, df1 = 1, df2 = 1)
\end{lstlisting}
\begin{lstlisting}[language=R, caption=n losowych zmiennych z rozkładu F]
rf(n, df1 = 1, df2 = 1)
\end{lstlisting}

\section{Metoda estymacji punktowej}

Jak dopasować rozkład i parametry do danych?

\begin{enumerate}
  \item Wybieramy $n$ próbek z danych ($X_1 \dots X_n$)
  \item Patrzymy na histogram i oceniamy vibe
  \item Dla parametrów wybieramy estymator, i przy pomocy estymatorów
    obliczamy parametry rozkładu
  \item Sprawdzamy, czy rozkład pasuje do danych
\end{enumerate}

Każdy estymator ma swój zakres ufności, z reguły określany przy pomocy wzoru.
Mając obliczony zakres ufności, możemy określić poziom ufności, czyli błąd
estymacji. Jeśli realna wartość parametru leży poza przedziałem ufności, to
sugeruje wadę w estymacji. Poziom ufności to procent prób, w których przedział
ufności zawiera prawdziwą wartość parametru.

\begin{lstlisting}[language=R, caption=estymuj parametry rozkładu normalnego]
  enorm(x, method = "mle", ci=TRUE, ci.type="two-sided", conf.level = 0.95)
\end{lstlisting}

\section{Metoda Monte Carlo}

Metoda Monte Carlo to metoda numeryczna, która polega na symulacji losowych
próbek z rozkładu i obliczeniu wartości funkcji na podstawie tych próbek. W
estymacji, na przykład, wystarczająco duża liczba próbek danych,
pozwala stworzyć
wykres wartości estymatora, który przybliża dystrybucje estymatora, co może
pozwolić na lepsze oszacowanie wartości parametru.

\textbf{Każda metoda, w której wykorzystujemy losowe próbki do
obliczenia wartości funkcji, to metoda Monte Carlo.}

\section{Bootstrapping}

Mając próbkę danych, możemy stworzyć wiele próbek z tej samej
populacji, z reguły poprzez losowanie z zwracaniem. Dla wystarczająco dużej
próbki początkowej w ten sposób możemy stworzyć wiele próbek, które
będą miały podobny rozkład do oryginalnej próbki. Próba stworzona w ten sposób
nazywana jest próbą bootsrapową.

\section{Testowanie hipotez}

Hipoteza statystyczne, to przypuszczenie dotyczące danych. Do weryfikacji
hipotez korzystamy z testów. Dla konkretnego testu wyznacza się poziom
istotności $\alpha$, który określa prawdopodobieństwo odrzucenia hipotezy
zerowej. Wielkość $1-\beta$ dla układu hipotez prostych nazywa się mocą testu
hipotezy zerowej wobec (prostej) hipotezy alternatywnej. Testy również są
parametryzowane przez $c$ czyli wartość krytyczną testu. W obecnie używanych
implementacjach komputerowych wartości krytyczne zastępowane są tzw.
p-wartościami (p-value), według pomysłu Ronalda Fishera. Jest to
prawdopodobieństwo wylosowania próby takiej lub bardziej skrajnej, jak
zaobserwowana przy założeniu, że hipoteza zerowa jest prawdziwa. Inaczej mówiąc,
jest to prawdopodobieństwo, że zależność, jaką otrzymaliśmy w próbie z populacji
mogła wystąpić przypadkowo, wskutek losowej zmienności, chociaż w populacji nie
występuje.

\section{Testy}

Konkretne algorytmy mające na celu weryfikację hipotez statystycznych.

\subsection{Test t-Studenta}

Test t-Studenta jest testem statystycznym, który służy do porównania
średnich dwóch grup, aby sprawdzić, czy są one statystycznie
różne.

\subsection{ANOVA}

ANOVA (analiza wariancji) jest testem statystycznym, który służy do
porównania średnich więcej niż dwóch grup, aby sprawdzić, czy są one
statystycznie różne. ANOVA jest rozszerzeniem testu t-Studenta, który
służy do porównania średnich dwóch grup.

\subsection{Test Shapiro-Wilka}

Test Shapiro-Wilka jest testem statystycznym, który służy do
sprawdzenia, czy dane pochodzą z rozkładu normalnego.

\subsection{F test}

F test jest testem statystycznym, który służy do porównania
wariancji dwóch grup, aby sprawdzić, czy są one
statystycznie różne.

\subsection{Testy statystycznej różności}

Wszystkie te testy służą do porównania średnich więcej niż dwóch grup, aby
sprawdzić, czy są one statystycznie różne. Różnią się one tym jak bardzo
są dokładne i konserwatywne.

\begin{itemize}
  \item HSD Tukeya
  \item LSD Fishera
  \item Test Studenta-Newmana-Keulsa
  \item Test Scheffego
  \item Test Duncana
  \item Test Dunnetta dla porównania z kontrolą
\end{itemize}

\section{Reference}

Kilka powszechnych celów oraz jak je osiągnąć w R.

\subsection{Korelacja}

Zupełnie jak $cov(x, y)$.

\begin{lstlisting}[language=R, caption=oblicz korelację]
cor(x, y, method = c("pearson", "kendall", "spearman"))
\end{lstlisting}

\subsection{Regresja}

Klasyczna regresja liniowa, czyli przyporządkowanie danym prostej, która
najlepiej je opisuje.

\begin{lstlisting}[language=R, caption=regresja liniowa]
lm(y ~ x, data = data.frame(x, y))
\end{lstlisting}

\begin{lstlisting}[language=R, caption=regresja wielomianowa]
lm(y ~ poly(x, degree = 2), data = data.frame(x, y))
\end{lstlisting}

\begin{lstlisting}[language=R, caption=różnice pomiędzy danymi a ich
  regresją]
residuals(lm(y ~ x, data = data.frame(x, y)))
\end{lstlisting}

\subsection{Szybka analiza danych}

Zwróci nam podsumowanie dot. danych, ich strukturę, pierwsze i ostatnie
wiersze oraz ich wymiary.
\begin{lstlisting}[language=R, caption=szybka analiza danych]
summary(data)
str(data)
head(data)
tail(data)
dim(data)
\end{lstlisting}

\subsection{Wizualizacja danych}

\begin{lstlisting}[language=R, caption=wykresy]
plot(x, y)
hist(x)
boxplot(x)
barplot(x)
pairs(data)
\end{lstlisting}

\subsection{Podział danych na k grup}

Przyporządkuje dane do k grup, w których każda grupa ma
jak najbliżej siebie elementy.
\begin{lstlisting}[language=R, caption=podział danych na k grup]
kmeans(data, centers = k)
\end{lstlisting}

\end{document}