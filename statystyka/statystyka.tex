\documentclass{../notatki}

\title{Statystyka}

\begin{document}

\tableofcontents

\section{Rozkłady}

\subsection{Rozkład dwumianowy}

Rozkład dwumianowy to rozkład sumy $n$ zmiennych losowych o rozkładzie
Bernoulliego. Zmienna losowa $X$ ma rozkład dwumianowy z parametrami $n$ i $p$,
zatem:
$$
P(X = k) = \binom{n}{k} p^k (1 - p)^{n-k}
$$
gdzie $\binom{n}{k}$ to liczba kombinacji $k$ sukcesów w $n$ próbach.

\subsection{Rozkład normalny}

Rozkład normalny (Gaussa) jest jednym z najważniejszych rozkładów
statystycznych. Jest on określony przez dwa parametry: wartość oczekiwaną
$\mu$ i wariancję $\sigma^2$. Gęstość rozkładu normalnego jest dana wzorem:
$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}
$$

\subsection{Rozkład chi-kwadrat}

Niech $X_1, X_2, \ldots, X_n$ będą niezależnymi zmiennymi losowymi o rozkładzie
normalnym $N(0, 1)$. Wtedy zmienna losowa $X = \sum_{i = 1}^{n} X_i^2$ ma
rozkład chi-kwadrat ($X \sim \chi(n)$) z $n$ stopniami swobody.
$$
f(x) =  \frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2 - 1} e^{-x/2}
$$

\subsection{Rozkład wykładniczy}

Jest to rozkład zmiennej, która opisuje czas między zdarzeniami w procesie
Poissona. Zmienna losowa $X$ ma rozkład wykładniczy z parametrem $\lambda$,
zatem:
$$
f(x, \lambda) =
\begin{cases}
  \lambda e^{-\lambda x} & \text{dla } x \geq 0 \\
  0 & \text{dla } x < 0
\end{cases}
$$

\subsection{Rozkład t-Studenta}

Niech $X \sim N(0, 1) oraz Y \sim \chi^2(n)$ będą niezależnymi
zmiennymi losowymi.
Wtedy zmienna losowa:
$$
\frac{X}{\sqrt{Y/n}}
$$
ma rozkład t-Studenta z $n$ stopniami swobody. Funkcja gęstości
rozkładu t-Studenta
jest dana wzorem:
$$
f(x, n) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi} \Gamma(\frac{n}{2})}
(1 + \frac{x^2}{n})^{-\frac{n+1}{2}}
$$

\subsection{F-Snedecora}

Niech $X \sim \chi^2(n)$, $Y \sim \chi^2(m)$ będą niezależnymi zmiennymi
losowymi. Wtedy zmienna losowa:
$$
\frac{X/n}{Y/m}
$$
ma rozkład F-Snedecora z $n$ i $m$ stopniami swobody. Funkcja gęstości
rozkładu F-Snedecora jest oznacza przez $F(n, m)$.

\section{Statystyka Opisowa}

Niech $X' = \{x_1, x_2, \ldots, x_n\}$ będzie zbiorem $n$ obserwacji zmiennej
losowej $X$. Zadaniem statystyki opisowej jest prezentacja rozkładu zmiennej
losowej $X$ w próbce $X'$.

\subsection{Rodzaje statystyk opisowe}

\begin{itemize}
  \item Klasyczne - uśredniające wartość próbki. Na przykład momenty
    zwykłe $r$-tego rzędu:
    $$
    m_r = \frac{1}{n} \sum_{i=1}^n x_i^r
    $$
  \item Pozycyjne - oparte na pozycjach obserwacji w próbce. Na przykład
    mediana, kwartyle, percentyle.
\end{itemize}

\subsection{Tendencja centralnej rozkładu empirycznego}

\begin{itemize}
  \item Średnia arytmetyczna:
    $$
    \overline{X'} = \frac{1}{n} \sum_{i=1}^n x_i
    $$
  \item Mediana
\end{itemize}

\subsection{Charakterystyki rozrzutu rozkładu empirycznego}

\begin{itemize}
  \item Odchylenie standardowe:
    $$
    s = \sqrt{\frac{1}{n - 1} \sum_{i=1}^n (x_i - \overline{X'})^2}
    $$
  \item Współczynnik zmienności:
    $$
    v = \frac{s}{\overline{X'}} \cdot 100\%
    $$
\end{itemize}

\section{Model statystyczny}

Jeżeli próba $X'$ jest reprezentatywna, to można na jej podstawie
wnioskować na temat populacji z której pochodzi. Aby określić zachowanie
zmiennej losowej $X$ w populacji, stosuje się model statystyczny.
Zatem traktujemy wektor $X'$ jako realizację zmiennej losowej $X$.

\section{Estymacja Punktowa}

Niech $X'$ będzie próba populacji o rozkładzie $P_\theta$ gdzie
$\theta \in \Theta$ jest parametrem. Estymatorem parametru $\theta$
nazywamy statystykę $\stackrel{\wedge}{\theta}: X' \rightarrow
\Theta$ która pozwala na
oszacowanie wartości parametru $\theta$.

\subsection{Metoda momentów}

Metoda momentów polega na przyrównaniu kolejnych $d$ momentów $m_1, \dots, m_d$
do odpowiednich momentów rozkładu populacji $E(X^i): i \in [1, d]$

\subsection{Metoda największej wiarygodności}

Funkcję $L(\theta, x) = p_\theta(x)$ nazywamy funkcją wiarygodności. Estymatorem
największej wiarygodności parametru $\theta$ nazywamy nazywamy statystykę
$\stackrel{\wedge}{\theta}$ która maksymalizuje funkcję wiarygodności.
$$
\forall_{x \in X} L(\stackrel{\wedge}{\theta}, x) = \sup_{\theta \in
\Theta} L(\theta, x)
$$

\subsection{Przykład}

Estymatorem największej wiarygodności oraz metody momentów dla rozkładu
wykładniczego z parametrem $\lambda$ jest:
$$
\stackrel{\wedge}{\lambda} = \frac{1}{\overline{X}}
$$

\subsection{Estymatory nieobciążone}

Estymator $\stackrel{\wedge}{\theta}$ nazywamy nieobciążonym, jeżeli
$E(\stackrel{\wedge}{\theta}) = \theta$

\subsection{Estymator modelu wykładniczego}

Dla modelu wykładniczego, parametryzowanego przez $\lambda$, estymatorem
nieobciążonym jest:
$$
\stackrel{\wedge}{\lambda} = \frac{n - 1}{n} \frac{1}{\overline{X}}
$$

\subsection{Estymator modelu normalnego}

Dla modelu normalnego, parametryzowanego przez $\mu$ i $\sigma^2$, estymatorem
nieobciążonym jest:
$$
\stackrel{\wedge}{\mu} = \overline{X}
$$
$$
\stackrel{\wedge}{\sigma^2} = S^2
$$

\subsection{Metoda monte carlo}

Niech $X'$ będzie próbą populacji o rozkładzie $P_\theta$, oraz niech
$\stackrel{\wedge}{\theta}$ będzie estymatorem parametru $\theta$. Załóżmy też,
że mamy $k$ niezależnych realizacji próby $x_1, \dots x_k$. Wtedy histogram
wartości $\stackrel{\wedge}{x_n}: n \in [1, k]$ jest przybliżeniem rozkładu
$\stackrel{\wedge}{\theta}$.

\subsection{Metoda bootstrap}

Dystrybuanta empiryczna to statystyka o następującej postaci:
$$
\stackrel{\wedge}{F}(x) = \frac{\#\{k: X_k \le x\}}{n}
$$
Dla takiej dystrybuanty i próby $X'$ zachodzi:
$$
\sup_{x \in \mathbb{R}} |\stackrel{\wedge}{F}(x) - F(x)| \xrightarrow{1} 0
$$
Próba bootstrapową $X^*$ to próba losowa z rozkładu empirycznego. Ta próba musi
powstać w wyniku $n$-krotnego losowania z zwracaniem. Rozkład statystyki
$T(X^*) - \stackrel{\wedge}{\theta}$ jest bliski rozkładowi statystyki
$T(X) - \theta$.

Mając $k$ realizacji prób bootstrapowych $X_1^*, \dots, X_k^*$, możemy
przybliżyć rozkład statystyki $\stackrel{\wedge}{\theta} - \theta$, poprzez
stworzenie histogramu $\stackrel{\wedge}{\theta}*_n: n\in [0,k]$

\section{Przedziały ufności}

Przedział ufności to przedział $[L, R]$ określony parą statystyk, takich, że:
$$
P_\theta(L < \theta < R) = 1 - \alpha
$$
gdzie $\alpha$ to poziom ufności, a $\theta$ to parametr modelu.

Funkcję $Q(X, \theta)$ nazywamy funkcją centralną dla parametru $\theta$, jeżeli
rozkład prawdopodobieństwa zmiennej $Q$ jest absolutnie ciągły i nie zależy od
parametru $\theta$, oraz funkcja $Q$ jest ciągła i ściśle monotoniczna względem
$\theta$.

Obieranie przedziału ufności następuje poprzez rozwiązanie nierówności:
$$
a < Q(X, \theta) < b
$$
gdzie $a$ i $b$ się z reguły dobiera tak aby:
$$
P(Q \le a) = P(Q \ge b) = \frac{\alpha}{2}
$$

\section{Test t-Studenta}

\subsection{Dla jednej próby}

\begin{itemize}
  \item \textbf{Hipoteza zerowa}: $H_0: \mu = \mu_0$
  \item Hipotezy alternatywne: $H_1: \mu \neq \mu_0$, $H_1: \mu >
    \mu_0$, $H_1: \mu < \mu_0$
  \item Statystyka testowa:
    $$
    t = \frac{\overline{X} - \mu_0}{S} \sqrt{n}
    $$
  \item Rozkład statystyki testowej: $t|_{H_0} \sim t(n-1)$
\end{itemize}

\subsection{Dla dwóch prób}

Posiadamy obserwacje jednej zmiennej (cechy) na jednostkach
eksperymentalnych pochodzących z dwóch populacji (grup) lub
posiadamy dwukrotne obserwacje tej samej zmiennej na tych samych
jednostkach eksperymentalnych jednej populacji. Wyróżniamy dwa
rodzaje prób: niezależne oraz zależne.

\subsubsection{Błąd}

Błąd to różnica między wartością zmiennej a wartością przewidywaną przez
model. Zakładamy, że błąd:
\begin{itemize}
  \item ma rozkład normalny
  \item jest niezależny od zmiennej
  \item ma wartość oczekiwaną równą 0
  \item ma stałą wariancję
\end{itemize}

\subsubsection{Próby niezależne z jednorodnymi wariancjami}

$$
\stackrel{\wedge}{\mu_1} = \overline{X}_1
$$
$$
\stackrel{\wedge}{\mu_2} = \overline{X}_2
$$
$$
\stackrel{\wedge}{\sigma^2} = S^2 = \frac{(n_1 - 1)S_1^2 + (n_2 -
1)S_2^2}{n_1 + n_2 - 2}
$$
$$
S_i^2 = \frac{1}{n_i - 1} \sum_{j=1}^{n_i} (x_{ij} - \overline{X}_i)^2
$$

\begin{itemize}
  \item \textbf{Hipoteza zerowa}: $H_0: \mu_1 = \mu_2$
  \item Hipotezy alternatywne: $H_1: \mu_1 \neq \mu_2$, $H_1: \mu_1 >
    \mu_2$, $H_1: \mu_1 < \mu_2$
  \item Statystyka testowa:
    $$
    t = \frac{\overline{X}_1 - \overline{X}_2}{S}
    \sqrt{\frac{n_1n_2}{n_1 + n_2}}
    $$
  \item Rozkład statystyki testowej: $t|_{H_0} \sim t(n_1 + n_2 - 2)$
\end{itemize}

\subsubsection{Próby niezależne z różnymi wariancjami}

$$
\stackrel{\wedge}{\mu_1} = \overline{X}_1
$$
$$
\stackrel{\wedge}{\mu_2} = \overline{X}_2
$$
$$
\stackrel{\wedge}{\sigma_1^2} = S_1^2 = \frac{1}{n_1 - 1}
\sum_{i=1}^{n_1} (x_{1i} - \overline{X}_1)^2
$$
$$
\stackrel{\wedge}{\sigma_2^2} = S_2^2
$$

\begin{itemize}
  \item \textbf{Hipoteza zerowa}: $H_0: \mu_1 = \mu_2$
  \item Hipotezy alternatywne: $H_1: \mu_1 \neq \mu_2$, $H_1: \mu_1 >
    \mu_2$, $H_1: \mu_1 < \mu_2$
  \item Statystyka testowa:
    $$
    t = \frac{\overline{X}_1 - \overline{X}_2}{\sqrt{\frac{S_1^2}{n_1} +
    \frac{S_2^2}{n_2}}}
    $$
  \item Rozkład statystyki testowej: $t|_{H_0} \sim t(m)$ (test Welch)
\end{itemize}

\section{Test F}

Będziemy zakładać dwie różne wariancje dla dwóch prób.

\begin{itemize}
  \item \textbf{Hipoteza zerowa}: $H_0: \sigma_1^2 = \sigma_2^2$
  \item Hipotezy alternatywne: $H_1: \sigma_1^2 \neq \sigma_2^2$, $H_1:
    \sigma_1^2 > \sigma_2^2$, $H_1: \sigma_1^2 < \sigma_2^2$
  \item Statystyka testowa:
    $$
    F = \frac{S_1^2}{S_2^2}
    $$
  \item Rozkład statystyki testowej: $F|_{H_0} \sim F(n_1 - 1, n_2 - 1)$
\end{itemize}

\end{document}
